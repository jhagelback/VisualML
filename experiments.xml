<?xml version="1.0"?>
<Experiments>
    <!--
        Linear Classifiers
        
        The following parameters are available:
        
        <Classifier>Linear</Classifier>                         Linear
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <Iterations>10</Iterations>                             Integer (default is 200)
        <LearningRate>1.0</LearningRate>                        Decimal value 0 ... 1 (default is 1.0)
        <UseRegularization>true</UseRegularization>             True or False (default is true)
        <RegularizationStrength>0.01</RegularizationStrength>   Decimal value (default is 0.01)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <BatchSize>100</BatchSize>                              Size of batches for batch training. If not set, batch training isn't used
    -->
    <Experiment id="l_demo">
        <!-- Training set: 100.00% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Iterations>10</Iterations>
        <LearningRate>1.0</LearningRate>
    </Experiment>
    <Experiment id="l_spiral">
        <!-- Training set: 49.00% -->
        <!-- Cross-validation: 36.00% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Iterations>200</Iterations>
        <LearningRate>0.1</LearningRate>
    </Experiment>
    <Experiment id="l_circle">
        <!-- Training set: 68.60% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Iterations>20</Iterations>
        <LearningRate>1.0</LearningRate>
    </Experiment>
    <Experiment id="l_iris">
        <!-- Training set: 98.00% -->
        <!-- Cross-validation: 94.67% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Iterations>300</Iterations>
        <LearningRate>0.1</LearningRate>
    </Experiment>
    <Experiment id="l_iris_test">
        <!-- Training set: 97.50% -->
        <!-- Test set: 93.33% -->
        <!-- Cross-validation: 97.50% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Iterations>300</Iterations>
        <LearningRate>0.1</LearningRate>
    </Experiment>
    <Experiment id="l_iris_2d">
        <!-- Training set: 96.00% -->
        <!-- Cross-validation: 95.33% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris.2d.csv</TrainingData>
        <Iterations>50</Iterations>
        <LearningRate>1.0</LearningRate>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="l_diabetes">
        <!-- Training set: 77.21% -->
        <!-- Cross-validation: 77.26% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Iterations>40</Iterations>
        <LearningRate>1.0</LearningRate>
        <Normalization>-1:1</Normalization>
    </Experiment>
    
    <Experiment id="l_glass">
        <!-- Training set: 56.54% -->
        <!-- Cross-validation: 58.87% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Iterations>100</Iterations>
        <LearningRate>0.4</LearningRate>
        <Normalization>0:2</Normalization>
    </Experiment>
    <Experiment id="l_mnist">
        <!-- Training set: 91.53% -->
        <!-- Test set: 91.68% -->
        <!-- Cross-validation: 92.26% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data_mnist/mnist_train.csv</TrainingData>
        <TestData>data_mnist/mnist_test.csv</TestData>
        <Iterations>1000</Iterations>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <Normalization>0:1</Normalization>
        <BatchSize>100</BatchSize>
    </Experiment>
    <!--
        Neural Network Classifiers
        
        The following parameters are available:
        
        <Classifier>NN</Classifier>                             NN
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <Iterations>10</Iterations>                             Integer (default is 1000)
        <LearningRate>1.0</LearningRate>                        Decimal value 0 ... 1 (default is 0.3)
        <UseRegularization>true</UseRegularization>             True or False (default is true)
        <RegularizationStrength>0.001</RegularizationStrength>   Decimal value (default is 0.001)
        <UseMomentum>true</UseMomentum>                         True or False (default is true)
        <HiddenLayers>16</HiddenLayers>                         Number of units in the hidden layer (default is 16)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <BatchSize>100</BatchSize>                              Size of batches for batch training. If not set, batch training isn't used
    -->
    <Experiment id="nn_demo">
        <!-- Training set: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Iterations>20</Iterations>
        <LearningRate>1.0</LearningRate>
        <HiddenLayers>8</HiddenLayers>
    </Experiment>
    <Experiment id="nn_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 91.33% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Iterations>8000</Iterations>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>72</HiddenLayers>
    </Experiment>
    <Experiment id="nn_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Iterations>100</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>16</HiddenLayers>
    </Experiment>
    <Experiment id="nn_iris">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Iterations>500</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>2</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="nn_iris_test">
        <!-- Training set: 99.17% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 97.50% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Iterations>500</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>2</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="nn_iris_2d">
        <!-- Training set: 96.00% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Iterations>200</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>2</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="nn_flame">
        <!-- Training set: 99.17% -->
        <!-- Cross-validation: 97.92% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Iterations>1200</Iterations>
        <LearningRate>0.5</LearningRate>
        <UseRegularization>true</UseRegularization>
        <HiddenLayers>16</HiddenLayers>
    </Experiment>
    <Experiment id="nn_jain">
        <!-- Training set: 95.17% -->
        <!-- Cross-validation: 89.58% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/jain.csv</TrainingData>
        <Iterations>300</Iterations>
        <LearningRate>0.8</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>16</HiddenLayers>
    </Experiment>
    <Experiment id="nn_diabetes">
        <!-- Training set: 88.80% -->
        <!-- Cross-validation: 78.96% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Iterations>12000</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>32</HiddenLayers>
        <Normalization>-1:1</Normalization>
        <BatchSize>100</BatchSize>
    </Experiment>
    <Experiment id="nn_glass">
        <!-- Training set: 94.39% -->
        <!-- Cross-validation: 86.01% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Iterations>4000</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>72</HiddenLayers>
        <Normalization>0:2</Normalization>
    </Experiment>
    <Experiment id="nn_mnist">
        <!-- Training set: 94.22% -->
        <!-- Test set: 93.88% -->
        <!-- Cross-validation: 96.06% -->
        <Classifier>NN</Classifier>
        <TrainingData>data_mnist/mnist_train.csv</TrainingData> <!-- 94.49% -->
        <TestData>data_mnist/mnist_test.csv</TestData> <!-- 94.43% -->
        <Iterations>2000</Iterations>
        <LearningRate>0.2</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>16</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <!--
        Deep Neural Network Classifiers
        
        The following parameters are available:
        
        <Classifier>NN</Classifier>                             NN
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <Iterations>10</Iterations>                             Integer (default is 1000)
        <LearningRate>1.0</LearningRate>                        Decimal value 0 ... 1 (default is 0.3)
        <UseRegularization>true</UseRegularization>             True or False (default is true)
        <RegularizationStrength>0.001</RegularizationStrength>   Decimal value (default is 0.001)
        <UseMomentum>true</UseMomentum>                         True or False (default is true)
        <HiddenLayers>16,8</HiddenLayers>                       Number of units in the hidden layers (default is 16)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
    -->
    <Experiment id="dnn_demo">
        <!-- Training set: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Iterations>100</Iterations>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>12,8</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 95.33% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Iterations>8000</Iterations>
        <LearningRate>0.1</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>42,24</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Iterations>200</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>12,8</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="dnn_iris">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 95.33% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Iterations>2000</Iterations>
        <LearningRate>0.8</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>8,4</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="dnn_iris_test">
        <!-- Training set: 99.17% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 99.17% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Iterations>2000</Iterations>
        <LearningRate>0.8</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>8,4</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="dnn_iris_2d">
        <!-- Training set: 96.00% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Iterations>100</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>8,4</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="dnn_flame">
        <!-- Training set: 99.17% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Iterations>1200</Iterations>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>12,8</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_jain">
        <!-- Training set: 95.17% -->
        <!-- Cross-validation: 92.70% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/jain.csv</TrainingData>
        <Iterations>300</Iterations>
        <LearningRate>0.7</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>12,8</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_diabetes">
        <!-- Training set: 98.05% -->
        <!-- Cross-validation: 80.91% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Iterations>10000</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>24,12</HiddenLayers>
        <Normalization>-1:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <Experiment id="dnn_glass">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 87.38% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Iterations>6000</Iterations>
        <LearningRate>0.8</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>64,32</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="dnn_mnist">
        <!-- Training set: 93.46% -->
        <!-- Test set: 92.97% -->
        <!-- Cross-validation: 94.41% -->
        <Classifier>NN</Classifier>
        <TrainingData>data_mnist/mnist_train.csv</TrainingData>
        <TestData>data_mnist/mnist_test.csv</TestData>
        <Iterations>2000</Iterations>
        <LearningRate>0.2</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>8,48</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <!--
        k-Nearest Neighbor Classifiers
        
        The following parameters are available:
        
        <Classifier>KNN</Classifier>                            KNN
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <K>3</K>                                                Integer (default is 3)
        <DistanceMeasure>L2</DistanceMeasure>                   L1 or L2 (default is L2)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
    -->
    <Experiment id="knn_demo">
        <!-- Training set: 100.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <K>2</K>
    </Experiment>
    <Experiment id="knn_spiral">
        <!-- Training set: 99.33% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 99.06% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_iris">
        <!-- Training set: 96.00% -->
        <!-- Cross-validation: 94.67% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_iris_test">
        <!-- Training set: 96.67% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 95.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_iris_2d">
        <!-- Training set: 98.00% -->
        <!-- Cross-validation: 98.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Normalization>-1:1</Normalization>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_flame">
        <!-- Training set: 99.58% -->
        <!-- Cross-validation: 98.33% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_jain">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 97.30% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/jain.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_diabetes">
        <!-- Training set: 85.81% -->
        <!-- Cross-validation: 74.56% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Normalization>-1:1</Normalization>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_glass">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 72.74% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <K>2</K>
    </Experiment>
    <Experiment id="knn_mnist">
        <Classifier>KNN</Classifier>
        <TrainingData>data_mnist/mnist_train.csv</TrainingData> <!-- 0.00% -->
        <TestData>data_mnist/mnist_test.csv</TestData> <!-- 0.00% -->
        <K>3</K>
    </Experiment>
    <!--
        RBF (Radial-Basis Function) Kernel Classifiers
        
        The following parameters are available:
        
        <Classifier>RBF</Classifier>                            RBF
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <Gamma>1.0</Gamma>                                      Gamma value for RBF kernel (decimal value, default is 3)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
    -->
    <Experiment id="rbf_demo">
        <!-- Training set: 100.00% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Gamma>1.0</Gamma>
    </Experiment>
    <Experiment id="rbf_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 36.67% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Gamma>40.0</Gamma>
    </Experiment>
    <Experiment id="rbf_circle">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 36.67% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Gamma>130.0</Gamma>
    </Experiment>
    <Experiment id="rbf_iris">
        <!-- Training set: 96.67% -->
        <!-- Cross-validation: 95.33% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Gamma>0.7</Gamma>
    </Experiment>
    <Experiment id="rbf_iris_test">
        <!-- Training set: 95.00% -->
        <!-- Test set: 100.00% -->
        <!-- Cross-validation: 94.17% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Gamma>0.5</Gamma>
    </Experiment>
    <Experiment id="rbf_iris_2d">
        <!-- Training set: 97.33% -->
        <!-- Cross-validation: 97.33% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Gamma>1.0</Gamma>
    </Experiment>
    <Experiment id="rbf_flame">
        <!-- Training set: 98.75% -->
        <!-- Cross-validation: 68.33% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Gamma>1000.0</Gamma>
    </Experiment>
    <Experiment id="rbf_jain">
        <!-- Training set: 98.12% -->
        <!-- Cross-validation: 84.32% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/jain.csv</TrainingData>
        <Gamma>55.0</Gamma>
    </Experiment>
    <Experiment id="rbf_diabetes">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 65.09% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Normalization>0:2</Normalization>
        <Gamma>100</Gamma>
    </Experiment>
    <Experiment id="rbf_glass">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 47.44% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Normalization>0:2</Normalization>
        <Gamma>200.0</Gamma>
    </Experiment>
</Experiments>