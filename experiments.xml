<?xml version="1.0"?>
<Experiments>
    <!--
        Linear Classifiers
        
        The following parameters are available:
        
        <Classifier>Linear</Classifier>                         Linear
        <TrainingData>data/demo.csv</TrainingData>              Path to training dataset
        <TestData></TestData>                                   Path to test dataset (or empty if no test data is used)
        <Epochs>10</Epochs>                                     Training epochs (default is 200)
        <LearningRate>1.0</LearningRate>                        Learning rate (default is 1.0)
        <StopThreshold>0.000005</StopThreshold>                 Loss threshold for when to stop training (default is 0.000005)
        <UseRegularization>true</UseRegularization>             Sets if regularization shall be used (default is true)
        <RegularizationStrength>0.01</RegularizationStrength>   Sets regularization strength (default is 0.01)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <BatchSize>100</BatchSize>                              Size of batches for batch training. If not set, batch training isn't used
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
    -->
    <Experiment id="l_demo">
        <!-- Training set: 100.00% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Epochs>10</Epochs>
        <LearningRate>1.0</LearningRate>
    </Experiment>
    <Experiment id="l_spiral">
        <!-- Training set: 56.67% -->
        <!-- Cross-validation: 54.67% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Epochs>100</Epochs>
        <LearningRate>0.1</LearningRate>
    </Experiment>
    <Experiment id="l_circle">
        <!-- Training set: 68.60% -->
        <!-- Cross-validation: 68.60% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Epochs>20</Epochs>
        <LearningRate>1.0</LearningRate>
    </Experiment>
    <Experiment id="l_iris">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 98.00% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Epochs>400</Epochs>
        <LearningRate>0.2</LearningRate>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="l_iris_pca">
        <!-- Training set: 98.00% -->
        <!-- Cross-validation: 98.00% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris_pca.csv</TrainingData>
        <Epochs>200</Epochs>
        <LearningRate>0.05</LearningRate>
    </Experiment>
    <Experiment id="l_iris_test">
        <!-- Training set: 97.50% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 97.50% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Epochs>400</Epochs>
        <LearningRate>0.1</LearningRate>
        <BatchSize>70</BatchSize>
    </Experiment>
    <Experiment id="l_iris_2d">
        <!-- Training set: 96.00% -->
        <!-- Cross-validation: 96.67% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Epochs>500</Epochs>
        <LearningRate>0.2</LearningRate>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="l_flame">
        <!-- Training set: 83.33% -->
        <!-- Cross-validation: 82.50% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Epochs>50</Epochs>
        <LearningRate>0.5</LearningRate>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="l_moons">
        <!-- Training set: 89.54% -->
        <!-- Cross-validation: 88.47% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/moons.csv</TrainingData>
        <Epochs>20</Epochs>
        <LearningRate>0.5</LearningRate>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="l_diabetes">
        <!-- Training set: 78.39% -->
        <!-- Cross-validation: 77.99% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Epochs>400</Epochs>
        <LearningRate>0.05</LearningRate>
        <UseRegularization>false</UseRegularization>
        <Normalization>-1:1</Normalization>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="l_diabetes_pca">
        <!-- Training set: 75.00% -->
        <!-- Cross-validation: 74.87% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/diabetes_pca.csv</TrainingData>
        <Epochs>400</Epochs>
        <LearningRate>0.2</LearningRate>
        <Normalization>-2:2</Normalization>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="l_glass">
        <!-- Training set: 63.55% -->
        <!-- Cross-validation: 58.41% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Epochs>500</Epochs>
        <LearningRate>0.05</LearningRate>
        <Normalization>0:2</Normalization>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="l_mnist">
        <!-- Training set: 93.19% -->
        <!-- Test set: 92.58% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data_mnist/mnist_train.csv.zip</TrainingData>
        <TestData>data_mnist/mnist_test.csv.zip</TestData>
        <Epochs>50</Epochs>
        <LearningRate>0.3</LearningRate>
        <UseRegularization>false</UseRegularization>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <Experiment id="l_mnist_pa">
        <!-- Training set: 92.54% -->
        <!-- Test set: 92.39% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data_mnist/mnist_train_scaled_avg.csv.zip</TrainingData>
        <TestData>data_mnist/mnist_test_scaled_avg.csv.zip</TestData>
        <Epochs>80</Epochs>
        <LearningRate>0.6</LearningRate>
        <UseRegularization>false</UseRegularization>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <!--
        Neural Network Classifiers
        
        The following parameters are available:
        
        <Classifier>NN</Classifier>                             NN
        <TrainingData>data/demo.csv</TrainingData>              Path to training dataset
        <TestData></TestData>                                   Path to test dataset (or empty if no test data is used)
        <Epochs>10</Epochs>                                     Training epochs (default is 200)
        <LearningRate>1.0</LearningRate>                        Learning rate (default is 1.0)
        <StopThreshold>0.000005</StopThreshold>                 Loss threshold for when to stop training (default is 0.000005)
        <UseRegularization>true</UseRegularization>             Sets if regularization shall be used (default is true)
        <RegularizationStrength>0.001</RegularizationStrength>  Sets regularization strength (default is 0.001)
        <UseMomentum>true</UseMomentum>                         Sets if momentum shall be used (default is true)
        <HiddenLayers>16</HiddenLayers>                         Number of units in the hidden layer (default is 16)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <BatchSize>100</BatchSize>                              Size of mini batches for batch training. If not set, full batch training is used
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
    -->
    <Experiment id="nn_demo">
        <!-- Training set: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Epochs>10</Epochs>
        <LearningRate>1.0</LearningRate>
        <HiddenLayers>8</HiddenLayers>
    </Experiment>
    <Experiment id="nn_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 97.67% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Epochs>800</Epochs>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>72</HiddenLayers>
        <BatchSize>100</BatchSize>
    </Experiment>
    <Experiment id="nn_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Epochs>10</Epochs>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>16</HiddenLayers>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="nn_iris">
        <!-- Training set: 98.00% -->
        <!-- Cross-validation: 97.33% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Epochs>200</Epochs>
        <LearningRate>1.0</LearningRate>
        <HiddenLayers>2</HiddenLayers>
        <Normalization>-1:1</Normalization>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="nn_iris_test">
        <!-- Training set: 99.17% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 98.33% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Epochs>500</Epochs>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>2</HiddenLayers>
        <Normalization>-1:1</Normalization>
        <BatchSize>70</BatchSize>
    </Experiment>
    <Experiment id="nn_iris_2d">
        <!-- Training set: 96.00% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Epochs>500</Epochs>
        <LearningRate>0.04</LearningRate>
        <HiddenLayers>4</HiddenLayers>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="nn_iris_pca">
        <!-- Training set: 98.00% -->
        <!-- Cross-validation: 98.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris_pca.csv</TrainingData>
        <Epochs>50</Epochs>
        <LearningRate>0.1</LearningRate>
        <HiddenLayers>24</HiddenLayers>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="nn_flame">
        <!-- Training set: 99.17% -->
        <!-- Cross-validation: 99.17% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Epochs>500</Epochs>
        <LearningRate>0.5</LearningRate>
        <HiddenLayers>16</HiddenLayers>
    </Experiment>
    <Experiment id="nn_moons">
        <!-- Training set: 95.98% -->
        <!-- Cross-validation: 95.71% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/moons.csv</TrainingData>
        <Epochs>1000</Epochs>
        <LearningRate>0.8</LearningRate>
        <HiddenLayers>16</HiddenLayers>
    </Experiment>
    <Experiment id="nn_diabetes">
        <!-- Training set: 81.12% -->
        <!-- Cross-validation: 77.21% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Epochs>450</Epochs>
        <LearningRate>0.2</LearningRate>
        <HiddenLayers>72</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="nn_diabetes_pca">
        <!-- Training set: 76.04% -->
        <!-- Cross-validation: 75.91% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/diabetes_pca.csv</TrainingData>
        <Epochs>1000</Epochs>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>16</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="nn_glass">
        <!-- Training set: 87.85% -->
        <!-- Cross-validation: 73.36% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Epochs>4000</Epochs> <!-- 600  4000 -->
        <LearningRate>0.5</LearningRate> <!-- 0.9  0.5 -->
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>72</HiddenLayers> <!-- 72 -->
        <Normalization>0:2</Normalization> <!-- 0:2 -->
        <BatchSize>50</BatchSize> <!-- 50 -->
    </Experiment>
    <Experiment id="nn_mnist">
        <!-- Training set: 98.13% -->
        <!-- Test set: 96.96% -->
        <Classifier>NN</Classifier>
        <TrainingData>data_mnist/mnist_train.csv.zip</TrainingData>
        <TestData>data_mnist/mnist_test.csv.zip</TestData>
        <Epochs>50</Epochs>
        <LearningRate>0.3</LearningRate>
        <HiddenLayers>32</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <Experiment id="nn_mnist_pa">
        <!-- 
            Using the scaled images has comparable results, 
            but reduces training and evaluation times by 75% 
        -->
        <!-- Training set: 97.23% -->
        <!-- Test set: 96.92% -->
        <Classifier>NN</Classifier>
        <TrainingData>data_mnist/mnist_train_scaled_avg.csv.zip</TrainingData>
        <TestData>data_mnist/mnist_test_scaled_avg.csv.zip</TestData>
        <Epochs>50</Epochs>
        <LearningRate>0.5</LearningRate>
        <HiddenLayers>32</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <!--
        Deep Neural Network Classifiers
        
        The following parameters are available:
        
        <Classifier>NN</Classifier>                             NN
        <TrainingData>data/demo.csv</TrainingData>              Path to training dataset
        <TestData></TestData>                                   Path to test dataset (or empty if no test data is used)
        <Epochs>10</Epochs>                                     Training epochs (default is 200)
        <LearningRate>1.0</LearningRate>                        Learning rate (default is 1.0)
        <StopThreshold>0.000005</StopThreshold>                 Loss threshold for when to stop training (default is 0.000005)
        <UseRegularization>true</UseRegularization>             Sets if regularization shall be used (default is true)
        <RegularizationStrength>0.001</RegularizationStrength>  Sets regularization strength (default is 0.001)
        <UseMomentum>true</UseMomentum>                         Sets if momentum shall be used (default is true)
        <HiddenLayers>16,8</HiddenLayers>                       Number of units in the hidden layers (default is 16)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <BatchSize>100</BatchSize>                              Size of batches for batch training. If not set, batch training isn't used
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
    -->
    <Experiment id="dnn_demo">
        <!-- Training set: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Epochs>10</Epochs>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>8,8</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 97.67% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Epochs>200</Epochs>
        <LearningRate>0.1</LearningRate>
        <HiddenLayers>42,24</HiddenLayers>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="dnn_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Epochs>10</Epochs>
        <LearningRate>1.0</LearningRate>
        <HiddenLayers>12,8</HiddenLayers>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="dnn_iris">
        <!-- Training set: 98.00% -->
        <!-- Cross-validation: 97.33% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Epochs>400</Epochs>
        <LearningRate>0.3</LearningRate>
        <HiddenLayers>4,4</HiddenLayers>
        <Normalization>-1:1</Normalization>
        <BatchSize>100</BatchSize>
    </Experiment>
    <Experiment id="dnn_iris_test">
        <!-- Training set: 99.17% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 96.67% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Epochs>300</Epochs>
        <LearningRate>0.4</LearningRate>
        <HiddenLayers>4,4</HiddenLayers>
        <Normalization>-1:1</Normalization>
        <BatchSize>100</BatchSize>
    </Experiment>
    <Experiment id="dnn_iris_2d">
        <!-- Training set: 96.67% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Epochs>1500</Epochs>
        <LearningRate>0.04</LearningRate>
        <HiddenLayers>8,8</HiddenLayers>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="dnn_iris_pca">
        <!-- Training set: 98.00% -->
        <!-- Cross-validation: 98.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris_pca.csv</TrainingData>
        <Epochs>400</Epochs>
        <LearningRate>0.02</LearningRate>
        <HiddenLayers>32,16</HiddenLayers>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="dnn_flame">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 98.75% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Epochs>500</Epochs>
        <LearningRate>0.3</LearningRate>
        <HiddenLayers>8,8</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_moons">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/moons.csv</TrainingData>
        <Epochs>700</Epochs>
        <LearningRate>0.6</LearningRate>
        <HiddenLayers>16,12</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_diabetes">
        <!-- Training set: 79.56% -->
        <!-- Cross-validation: 76.43% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Epochs>200</Epochs>
        <LearningRate>0.6</LearningRate>
        <HiddenLayers>8,8</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>100</BatchSize>
    </Experiment>
    <Experiment id="dnn_diabetes_pca">
        <!-- Training set: 75.78% -->
        <!-- Cross-validation: 75.13% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/diabetes_pca.csv</TrainingData>
        <Epochs>400</Epochs>
        <LearningRate>0.2</LearningRate>
        <StopThreshold>0.000001</StopThreshold>
        <HiddenLayers>8,8</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="dnn_glass">
        <!-- Training set: 99.07% -->
        <!-- Cross-validation: 71.96% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Epochs>3000</Epochs>
        <LearningRate>0.2</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>64,64</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>50</BatchSize>
    </Experiment>
    <Experiment id="dnn_mnist">
        <!-- Training set: 98.62%-->
        <!-- Test set: 96.78% -->
        <Classifier>NN</Classifier>
        <TrainingData>data_mnist/mnist_train.csv.zip</TrainingData>
        <TestData>data_mnist/mnist_test.csv.zip</TestData>
        <Epochs>80</Epochs>
        <LearningRate>0.2</LearningRate>
        <HiddenLayers>32,16</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <Experiment id="dnn_mnist_pa">
        <!-- Training set: 98.57%-->
        <!-- Test set: 97.71% -->
        <Classifier>NN</Classifier>
        <TrainingData>data_mnist/mnist_train_scaled_avg.csv.zip</TrainingData>
        <TestData>data_mnist/mnist_test_scaled_avg.csv.zip</TestData>
        <Epochs>70</Epochs>
        <LearningRate>0.5</LearningRate>
        <HiddenLayers>56,16</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <!--
        k-Nearest Neighbor Classifiers
        
        The following parameters are available:
        
        <Classifier>KNN</Classifier>                            KNN
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <K>3</K>                                                Integer (default is 3)
        <DistanceMeasure>L2</DistanceMeasure>                   L1 or L2 (default is L2)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
    -->
    <Experiment id="knn_demo">
        <!-- Training set: 100.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <K>2</K>
    </Experiment>
    <Experiment id="knn_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 98.33% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_iris">
        <!-- Training set: 96.67% -->
        <!-- Cross-validation: 95.33% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <K>4</K>
    </Experiment>
    <Experiment id="knn_iris_test">
        <!-- Training set: 97.50% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 95.83% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <K>4</K>
    </Experiment>
    <Experiment id="knn_iris_2d">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <K>4</K>
    </Experiment>
    <Experiment id="knn_iris_pca">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris_pca.csv</TrainingData>
        <K>4</K>
    </Experiment>
    <Experiment id="knn_flame">
        <!-- Training set: 99.58% -->
        <!-- Cross-validation: 98.33% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_moons">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/moons.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_diabetes">
        <!-- Training set: 85.81% -->
        <!-- Cross-validation: 73.05% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Normalization>-1:1</Normalization>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_diabetes_pca">
        <!-- Training set: 84.51% -->
        <!-- Cross-validation: 69.40% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/diabetes_pca.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_glass">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 72.43% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <K>2</K>
    </Experiment>
    <Experiment id="knn_mnist">
         <!-- Training set: 99.02% -->
        <!-- Test set: 97.43% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data_mnist/mnist_train_scaled_avg.csv.zip</TrainingData>
        <TestData>data_mnist/mnist_test_scaled_avg.csv.zip</TestData>
        <K>3</K>
    </Experiment>
    <!--
        RBF (Radial-Basis Function) Kernel Classifiers
        
        The following parameters are available:
        
        <Classifier>RBF</Classifier>                            RBF
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <Gamma>1.0</Gamma>                                      Gamma value for RBF kernel (decimal value, default is 3)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
    -->
    <Experiment id="rbf_demo">
        <!-- Training set: 100.00% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Gamma>1.0</Gamma>
    </Experiment>
    <Experiment id="rbf_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 97.00% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Gamma>40.0</Gamma>
    </Experiment>
    <Experiment id="rbf_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 84.11% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Gamma>130.0</Gamma>
    </Experiment>
    <Experiment id="rbf_iris">
        <!-- Training set: 96.67% -->
        <!-- Cross-validation: 94.00% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Gamma>0.7</Gamma>
    </Experiment>
    <Experiment id="rbf_iris_test">
        <!-- Training set: 95.00% -->
        <!-- Test set: 100.00% -->
        <!-- Cross-validation: 92.50% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Gamma>0.5</Gamma>
    </Experiment>
    <Experiment id="rbf_iris_2d">
        <!-- Training set: 97.33% -->
        <!-- Cross-validation: 97.33% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Gamma>1.0</Gamma>
    </Experiment>
    <Experiment id="rbf_iris_pca">
        <!-- Training set: 96.00% -->
        <!-- Cross-validation: 94.67% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris_pca.csv</TrainingData>
        <Gamma>1.5</Gamma>
    </Experiment>
    <Experiment id="rbf_flame">
        <!-- Training set: 98.33% -->
        <!-- Cross-validation: 88.75% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Gamma>1000.0</Gamma>
    </Experiment>
    <Experiment id="rbf_moons">
        <!-- Training set: 98.12% -->
        <!-- Cross-validation: 97.86% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/moons.csv</TrainingData>
        <Gamma>55.0</Gamma>
    </Experiment>
    <Experiment id="rbf_diabetes">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 65.10% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Normalization>0:2</Normalization>
        <Gamma>100</Gamma>
    </Experiment>
    <Experiment id="rbf_diabetes_pca">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 65.10% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/diabetes_pca.csv</TrainingData>
        <Gamma>10</Gamma>
    </Experiment>
    <Experiment id="rbf_glass">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 46.26% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Normalization>0:2</Normalization>
        <Gamma>200.0</Gamma>
    </Experiment>
    <!--
        CART Tree Classifiers
        
        The following parameters are available:
        
        <Classifier>CART</Classifier>                           CART
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <MaxDepth>5</MaxDepth>                                  Max depth of the tree (decimal value, default is 5)
        <MinSize>10</MinSize>                                   Minimum size of dataset for a split (decimal value, default is 10)
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
    -->
    <Experiment id="c_demo">
        <!-- Training set: 100.00% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <MaxDepth>2</MaxDepth>
        <MinSize>1</MinSize>
    </Experiment>
    <Experiment id="c_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 94.33% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <MaxDepth>7</MaxDepth>
        <MinSize>5</MinSize>
    </Experiment>
    <Experiment id="c_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 98.06% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>20</MinSize>
    </Experiment>
    <Experiment id="c_iris">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 94.00% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
    </Experiment>
    <Experiment id="c_iris_pca">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/iris_pca.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>10</MinSize>
    </Experiment>
    <Experiment id="c_iris_svd">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/iris_svd.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>10</MinSize>
    </Experiment>
    <Experiment id="c_iris_test">
        <!-- Training set: 97.50% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 92.50% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
    </Experiment>
    <Experiment id="c_iris_2d">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 94.67% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
        <Normalization>0:1</Normalization>
    </Experiment>
    <Experiment id="c_flame">
        <!-- Training set: 99.17% -->
        <!-- Cross-validation: 96.67% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
    </Experiment>
    <Experiment id="c_moons">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 98.93% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/moons.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
    </Experiment>
    <Experiment id="c_diabetes">
        <!-- Training set: 83.46% -->
        <!-- Cross-validation: 75.65% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>10</MinSize>
    </Experiment>
    <Experiment id="c_diabetes_pca">
        <!-- Training set: 78.26% -->
        <!-- Cross-validation: 69.27% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/diabetes_pca.csv</TrainingData>
        <MaxDepth>4</MaxDepth>
        <MinSize>10</MinSize>
    </Experiment>
    <Experiment id="c_glass">
        <!-- Training set: 87.85% -->
        <!-- Cross-validation: 66.82% -->
        <Classifier>CART</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <MaxDepth>7</MaxDepth>
        <MinSize>5</MinSize>
    </Experiment>
    <Experiment id="c_mnist">
        <!-- Training set: 0.00% -->
        <!-- Test set: 0.00% -->
        <!-- Cross-validation: 0.00% -->
        <Classifier>CART</Classifier>
        <TrainingData>data_mnist/mnist_train.csv</TrainingData>
        <TestData>data_mnist/mnist_test.csv</TestData>
        <MaxDepth>5</MaxDepth>
        <MinSize>10</MinSize>
    </Experiment>
    <!--
        Random Forest Classifiers
        
        The following parameters are available:
        
        <Classifier>RF</Classifier>                             RF
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <MaxDepth>5</MaxDepth>                                  Max depth of the tree (decimal value, default is 5)
        <MinSize>10</MinSize>                                   Minimum size of dataset for a split (decimal value, default is 10)
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
        <Trees>7</Trees>                                       Number of trees in the forest (default is 7)
        <SampleSize>0.9</SampleSize>                            Sample size of data subset for each tree (default is 0.9) 
    -->
    
    <Experiment id="rf_demo">
        <!-- Training set: 100.00% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <MaxDepth>2</MaxDepth>
        <MinSize>1</MinSize>
        <Trees>3</Trees>
        <SampleSize>0.8</SampleSize>
    </Experiment>
    <Experiment id="rf_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <MaxDepth>7</MaxDepth>
        <MinSize>5</MinSize>
        <Trees>11</Trees>
    </Experiment>
    <Experiment id="rf_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 98.45% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
        <Trees>7</Trees>
    </Experiment>
    <Experiment id="rf_iris">
        <!-- Training set: 97.33% -->
        <!-- Cross-validation: 96.67% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
        <Trees>11</Trees>
    </Experiment>
    <Experiment id="rf_iris_pca">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 96.67% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/iris_pca.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
        <Trees>25</Trees>
    </Experiment>
    <Experiment id="rf_iris_test">
        <!-- Training set: 98.33% -->
        <!-- Test set: 100.00% -->
        <!-- Cross-validation: 95.00% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
        <Trees>15</Trees>
    </Experiment>
    <Experiment id="rf_iris_2d">
        <!-- Training set: 98.00% -->
        <!-- Cross-validation: 94.67% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <MaxDepth>5</MaxDepth>
        <MinSize>5</MinSize>
        <Trees>15</Trees>
    </Experiment>
    <Experiment id="rf_flame">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 98.75% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <MaxDepth>6</MaxDepth>
        <MinSize>5</MinSize>
        <Trees>19</Trees>
    </Experiment>
    <Experiment id="rf_moons">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 98.12% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/moons.csv</TrainingData>
        <MaxDepth>7</MaxDepth>
        <MinSize>3</MinSize>
        <Trees>7</Trees>
    </Experiment>
    <Experiment id="rf_diabetes">
        <!-- Training set: 89.58% -->
        <!-- Cross-validation: 77.99% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <MaxDepth>7</MaxDepth>
        <MinSize>10</MinSize>
        <Trees>19</Trees>
    </Experiment>
    <Experiment id="rf_diabetes_pca">
        <!-- Training set: 84.77% -->
        <!-- Cross-validation: 71.48% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/diabetes_pca.csv</TrainingData>
        <MaxDepth>7</MaxDepth>
        <MinSize>10</MinSize>
        <Trees>19</Trees>
    </Experiment>
    <Experiment id="rf_glass">
        <!-- Training set: 94.39% -->
        <!-- Cross-validation: 74.30% -->
        <Classifier>RF</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <MaxDepth>7</MaxDepth>
        <MinSize>5</MinSize>
        <Trees>25</Trees>
    </Experiment>
</Experiments>