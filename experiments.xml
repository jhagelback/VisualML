<?xml version="1.0"?>
<Experiments>
    <!--
        Linear Classifiers
        
        The following parameters are available:
        
        <Classifier>Linear</Classifier>                         Linear
        <TrainingData>data/demo.csv</TrainingData>              Path to training dataset
        <TestData></TestData>                                   Path to test dataset (or empty if no test data is used)
        <Iterations>10</Iterations>                             Training iterations (default is 200)
        <LearningRate>1.0</LearningRate>                        Learning rate (default is 1.0)
        <UseRegularization>true</UseRegularization>             Sets if regularization shall be used (default is true)
        <RegularizationStrength>0.01</RegularizationStrength>   Sets regularization strength (default is 0.01)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <BatchSize>100</BatchSize>                              Size of batches for batch training. If not set, batch training isn't used
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
    -->
    <Experiment id="l_demo">
        <!-- Training set: 100.00% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Iterations>10</Iterations>
        <LearningRate>1.0</LearningRate>
    </Experiment>
    <Experiment id="l_spiral">
        <!-- Training set: 49.00% -->
        <!-- Cross-validation: 49.33% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Iterations>200</Iterations>
        <LearningRate>0.1</LearningRate>
    </Experiment>
    <Experiment id="l_circle">
        <!-- Training set: 68.60% -->
        <!-- Cross-validation: 68.67% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Iterations>20</Iterations>
        <LearningRate>1.0</LearningRate>
    </Experiment>
    <Experiment id="l_iris">
        <!-- Training set: 98.00% -->
        <!-- Cross-validation: 98.00% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Iterations>1000</Iterations>
        <LearningRate>0.2</LearningRate>
    </Experiment>
    <Experiment id="l_iris_test">
        <!-- Training set: 97.50% -->
        <!-- Test set: 93.33% -->
        <!-- Cross-validation: 97.50% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Iterations>500</Iterations>
        <LearningRate>0.1</LearningRate>
    </Experiment>
    <Experiment id="l_iris_2d">
        <!-- Training set: 96.00% -->
        <!-- Cross-validation: 95.33% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/iris.2d.csv</TrainingData>
        <Iterations>500</Iterations>
        <LearningRate>0.1</LearningRate>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="l_flame">
        <!-- Training set: 67.92% -->
        <!-- Cross-validation: 68.33% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Iterations>200</Iterations>
        <LearningRate>0.5</LearningRate>
    </Experiment>
    <Experiment id="l_jain">
        <!-- Training set: 85.79% -->
        <!-- Cross-validation: 85.01% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/jain.csv</TrainingData>
        <Iterations>200</Iterations>
        <LearningRate>0.5</LearningRate>
    </Experiment>
    <Experiment id="l_diabetes">
        <!-- Training set: 77.08% -->
        <!-- Cross-validation: 76.64% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Iterations>500</Iterations>
        <LearningRate>0.1</LearningRate>
        <Normalization>-1:1</Normalization>
    </Experiment>
    
    <Experiment id="l_glass">
        <!-- Training set: 56.54% -->
        <!-- Cross-validation: 55.52% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Iterations>500</Iterations>
        <LearningRate>0.1</LearningRate>
        <Normalization>0:2</Normalization>
    </Experiment>
    <Experiment id="l_mnist">
        <!-- Training set: 91.52% -->
        <!-- Test set: 91.65% -->
        <!-- Cross-validation: 91.06% -->
        <Classifier>Linear</Classifier>
        <TrainingData>data_mnist/mnist_train.csv</TrainingData>
        <TestData>data_mnist/mnist_test.csv</TestData>
        <Iterations>1000</Iterations>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <Normalization>0:1</Normalization>
        <BatchSize>100</BatchSize>
    </Experiment>
    <!--
        Neural Network Classifiers
        
        The following parameters are available:
        
        <Classifier>NN</Classifier>                             NN
        <TrainingData>data/demo.csv</TrainingData>              Path to training dataset
        <TestData></TestData>                                   Path to test dataset (or empty if no test data is used)
        <Iterations>10</Iterations>                             Training iterations (default is 200)
        <LearningRate>1.0</LearningRate>                        Learning rate (default is 1.0)
        <UseRegularization>true</UseRegularization>             Sets if regularization shall be used (default is true)
        <RegularizationStrength>0.001</RegularizationStrength>  Sets regularization strength (default is 0.001)
        <UseMomentum>true</UseMomentum>                         Sets if momentum shall be used (default is true)
        <HiddenLayers>16</HiddenLayers>                         Number of units in the hidden layer (default is 16)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <BatchSize>100</BatchSize>                              Size of batches for batch training. If not set, batch training isn't used
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
    -->
    <Experiment id="nn_demo">
        <!-- Training set: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Iterations>20</Iterations>
        <LearningRate>1.0</LearningRate>
        <HiddenLayers>8</HiddenLayers>
    </Experiment>
    <Experiment id="nn_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 98.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Iterations>8000</Iterations>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>72</HiddenLayers>
    </Experiment>
    <Experiment id="nn_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Iterations>100</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>16</HiddenLayers>
    </Experiment>
    <Experiment id="nn_iris">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 98.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Iterations>1000</Iterations>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>8</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="nn_iris_test">
        <!-- Training set: 99.17% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 97.50% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Iterations>500</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>2</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="nn_iris_2d">
        <!-- Training set: 96.00% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Iterations>1000</Iterations>
        <LearningRate>0.3</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>6</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="nn_flame">
        <!-- Training set: 99.17% -->
        <!-- Cross-validation: 98.33% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Iterations>1200</Iterations>
        <LearningRate>0.5</LearningRate>
        <UseRegularization>true</UseRegularization>
        <HiddenLayers>16</HiddenLayers>
    </Experiment>
    <Experiment id="nn_jain">
        <!-- Training set: 95.44% -->
        <!-- Cross-validation: 95.20% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/jain.csv</TrainingData>
        <Iterations>300</Iterations>
        <LearningRate>0.8</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>16</HiddenLayers>
    </Experiment>
    <Experiment id="nn_diabetes">
        <!-- Training set: 83.07% -->
        <!-- Cross-validation: 77.02% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Iterations>2500</Iterations>
        <LearningRate>0.9</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>32</HiddenLayers>
        <Normalization>-1:1</Normalization>
        <ShuffleData>false</ShuffleData>       
    </Experiment>
    <Experiment id="nn_glass">
        <!-- Training set: 98.13% -->
        <!-- Cross-validation: 71.26% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Iterations>6000</Iterations> <!-- 6000 -->
        <LearningRate>0.9</LearningRate> <!-- 1.0 -->
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>72</HiddenLayers> <!-- 72 -->
        <Normalization>0:2</Normalization> <!-- 0:2 -->
    </Experiment>
    <Experiment id="nn_mnist">
        <!-- Training set: 96.50% -->
        <!-- Test set: 95.96% -->
        <!-- Cross-validation: 95.74% -->
        <Classifier>NN</Classifier>
        <TrainingData>data_mnist/mnist_train.csv</TrainingData>
        <TestData>data_mnist/mnist_test.csv</TestData>
        <Iterations>2000</Iterations>
        <LearningRate>0.3</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>32</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <!--
        Deep Neural Network Classifiers
        
        The following parameters are available:
        
        <Classifier>NN</Classifier>                             NN
        <TrainingData>data/demo.csv</TrainingData>              Path to training dataset
        <TestData></TestData>                                   Path to test dataset (or empty if no test data is used)
        <Iterations>10</Iterations>                             Training iterations (default is 200)
        <LearningRate>1.0</LearningRate>                        Learning rate (default is 1.0)
        <UseRegularization>true</UseRegularization>             Sets if regularization shall be used (default is true)
        <RegularizationStrength>0.001</RegularizationStrength>  Sets regularization strength (default is 0.001)
        <UseMomentum>true</UseMomentum>                         Sets if momentum shall be used (default is true)
        <HiddenLayers>16,8</HiddenLayers>                       Number of units in the hidden layers (default is 16)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
        <BatchSize>100</BatchSize>                              Size of batches for batch training. If not set, batch training isn't used
        <ShuffleData>true</ShuffleData>                         Sets if dataset shall be shuffle (default is true)
    -->
    <Experiment id="dnn_demo">
        <!-- Training set: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Iterations>100</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>12,8</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 98.33% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Iterations>8000</Iterations>
        <LearningRate>0.1</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>42,24</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Iterations>1000</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>12,8</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="dnn_iris">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 98.00% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Iterations>500</Iterations>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>8,4</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="dnn_iris_test">
        <!-- Training set: 99.17% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 98.33% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Iterations>1000</Iterations>
        <LearningRate>0.6</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>8,4</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="dnn_iris_2d">
        <!-- Training set: 96.67% -->
        <!-- Cross-validation: 96.67% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Iterations>400</Iterations>
        <LearningRate>0.6</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>16,8</HiddenLayers>
        <Normalization>-1:1</Normalization>
        <BatchSize>100</BatchSize>
    </Experiment>
    <Experiment id="dnn_flame">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 99.17% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Iterations>2000</Iterations>
        <LearningRate>0.4</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>12,8</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_jain">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 97.32% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/jain.csv</TrainingData>
        <Iterations>2500</Iterations>
        <LearningRate>0.7</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>16,12</HiddenLayers>
    </Experiment>
    <Experiment id="dnn_diabetes">
        <!-- Training set: 98.31% -->
        <!-- Cross-validation: 71.54% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Iterations>4000</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>64,32</HiddenLayers>
        <Normalization>-1:1</Normalization>
        <ShuffleData>false</ShuffleData>
    </Experiment>
    <Experiment id="dnn_glass">
        <!-- Training set: 98.13% -->
        <!-- Cross-validation: 69.26% -->
        <Classifier>NN</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Iterations>3000</Iterations>
        <LearningRate>1.0</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>64,32</HiddenLayers>
        <Normalization>-1:1</Normalization>
    </Experiment>
    <Experiment id="dnn_mnist">
        <!-- Training set: 97.32% -->
        <!-- Test set: 96.17% -->
        <!-- Cross-validation: 96.13% -->
        <Classifier>NN</Classifier>
        <TrainingData>data_mnist/mnist_train.csv</TrainingData>
        <TestData>data_mnist/mnist_test.csv</TestData>
        <Iterations>3000</Iterations>
        <LearningRate>0.3</LearningRate>
        <UseRegularization>false</UseRegularization>
        <HiddenLayers>32,16</HiddenLayers>
        <Normalization>0:1</Normalization>
        <BatchSize>200</BatchSize>
    </Experiment>
    <!--
        k-Nearest Neighbor Classifiers
        
        The following parameters are available:
        
        <Classifier>KNN</Classifier>                            KNN
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <K>3</K>                                                Integer (default is 3)
        <DistanceMeasure>L2</DistanceMeasure>                   L1 or L2 (default is L2)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
    -->
    <Experiment id="knn_demo">
        <!-- Training set: 100.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <K>2</K>
    </Experiment>
    <Experiment id="knn_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 98.33% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_iris">
        <!-- Training set: 96.67% -->
        <!-- Cross-validation: 95.33% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <K>4</K>
    </Experiment>
    <Experiment id="knn_iris_test">
        <!-- Training set: 97.50% -->
        <!-- Test set: 96.67% -->
        <!-- Cross-validation: 95.83% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <K>4</K>
    </Experiment>
    <Experiment id="knn_iris_2d">
        <!-- Training set: 98.67% -->
        <!-- Cross-validation: 96.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Normalization>-1:1</Normalization>
        <K>4</K>
    </Experiment>
    <Experiment id="knn_flame">
        <!-- Training set: 99.58% -->
        <!-- Cross-validation: 98.33% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_jain">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 100.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/jain.csv</TrainingData>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_diabetes">
        <!-- Training set: 85.81% -->
        <!-- Cross-validation: 73.00% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Normalization>-1:1</Normalization>
        <K>3</K>
    </Experiment>
    <Experiment id="knn_glass">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 72.44% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <K>2</K>
    </Experiment>
    <Experiment id="knn_mnist">
         <!-- Training set: 98.87% -->
        <!-- Test set: 97.17% -->
        <Classifier>KNN</Classifier>
        <TrainingData>data_mnist/mnist_train.csv</TrainingData>
        <TestData>data_mnist/mnist_test.csv</TestData>
        <K>3</K>
    </Experiment>
    <!--
        RBF (Radial-Basis Function) Kernel Classifiers
        
        The following parameters are available:
        
        <Classifier>RBF</Classifier>                            RBF
        <TrainingData>data/demo.csv</TrainingData>              Path
        <TestData></TestData>                                   Path (or empty if no test data is used)
        <Gamma>1.0</Gamma>                                      Gamma value for RBF kernel (decimal value, default is 3)
        <Normalization>0:1</Normalization>                      Lower and upper bound for normalized values
    -->
    <Experiment id="rbf_demo">
        <!-- Training set: 100.00% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/demo.csv</TrainingData>
        <Gamma>1.0</Gamma>
    </Experiment>
    <Experiment id="rbf_spiral">
        <!-- Training set: 99.33% -->
        <!-- Cross-validation: 97.00% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/spiral.csv</TrainingData>
        <Gamma>40.0</Gamma>
    </Experiment>
    <Experiment id="rbf_circle">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 84.28% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/circle.csv</TrainingData>
        <Gamma>130.0</Gamma>
    </Experiment>
    <Experiment id="rbf_iris">
        <!-- Training set: 96.67% -->
        <!-- Cross-validation: 94.00% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris.csv</TrainingData>
        <Gamma>0.7</Gamma>
    </Experiment>
    <Experiment id="rbf_iris_test">
        <!-- Training set: 95.00% -->
        <!-- Test set: 100.00% -->
        <!-- Cross-validation: 92.50% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris_training.csv</TrainingData>
        <TestData>data/iris_test.csv</TestData>
        <Gamma>0.5</Gamma>
    </Experiment>
    <Experiment id="rbf_iris_2d">
        <!-- Training set: 97.33% -->
        <!-- Cross-validation: 97.33% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/iris.2D.csv</TrainingData>
        <Gamma>1.0</Gamma>
    </Experiment>
    <Experiment id="rbf_flame">
        <!-- Training set: 98.75% -->
        <!-- Cross-validation: 89.17% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/flame.csv</TrainingData>
        <Gamma>1000.0</Gamma>
    </Experiment>
    <Experiment id="rbf_jain">
        <!-- Training set: 98.12% -->
        <!-- Cross-validation: 97.84% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/jain.csv</TrainingData>
        <Gamma>55.0</Gamma>
    </Experiment>
    <Experiment id="rbf_diabetes">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 65.13% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/diabetes.csv</TrainingData>
        <Normalization>0:2</Normalization>
        <Gamma>100</Gamma>
    </Experiment>
    <Experiment id="rbf_glass">
        <!-- Training set: 100.00% -->
        <!-- Cross-validation: 46.46% -->
        <Classifier>RBF</Classifier>
        <TrainingData>data/glass.csv</TrainingData>
        <Normalization>0:2</Normalization>
        <Gamma>200.0</Gamma>
    </Experiment>
</Experiments>